{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.linalg import *\n",
    "np.random.seed(42)  # don't change this line\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import optimize, stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_fit(dist, data, method=\"mle\"):\n",
    "    data = np.asarray(data)\n",
    "    start = dist._fitstart(data)\n",
    "    args = [start[0:-2], (start[-2], start[-1])]\n",
    "    x0, func, restore, args = dist._reduce_func(args, {}, data=data)\n",
    "    vals = optimize.fmin(func, x0, args=(np.ravel(data),))\n",
    "    return vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "class LinearRegression:\n",
    "\n",
    "    \n",
    "    def __init__(self, alpha = 0.01, tol=1e-4, max_iter = 100, theta_init = None, penalty = None, lambd = 0):\n",
    "        \n",
    "        # store meta-data\n",
    "        self.alpha = alpha\n",
    "        self.theta_init = theta_init\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.penalty = penalty\n",
    "        self.lambd = lambd\n",
    "\n",
    "        self.theta_ = None\n",
    "        self.hist_cost_ = None\n",
    "        self.hist_theta_ = None\n",
    "    \n",
    "    # TODO STARTS: Complete the function (should account for three cases - no penalty, l1 penalty, and l2 penalty)\n",
    "    def compute_cost(self, theta, X, y):\n",
    "          totalCost = 0\n",
    "          D = len(theta)\n",
    "          for i in range(len(X)):\n",
    "            h = np.dot(np.transpose(theta), X[i])\n",
    "            totalCost += (h-y[i])** 2\n",
    "          totalCost /= len(X)\n",
    "          if self.penalty == \"l1\":\n",
    "            sum = 0\n",
    "            for i in range(1, D):\n",
    "              sum = sum + np.abs(theta[i]) \n",
    "            totalCost = totalCost + self.lambd * sum\n",
    "          elif self.penalty == \"l2\":\n",
    "            sum = 0\n",
    "            for i in range(1, D):\n",
    "              sum = sum + np.square(theta[i]) \n",
    "            totalCost = totalCost + self.lambd * sum\n",
    "            return totalCost\n",
    "        # TODO ENDS\n",
    "\n",
    "    def compute_gradient(self, theta, X, y):\n",
    "   \n",
    "        \n",
    "        # TODO STARTS: Complete the function (should account for three cases - no penalty, l1 penalty, and l2 penalty)\n",
    "        return np.gradient(self.compute_cost(theta, X, y))\n",
    "        # TODO ENDS\n",
    "\n",
    "    def has_converged(self, theta_old, theta_new):\n",
    "            converged = False\n",
    "            grad = self.compute_gradient(self, theta, X, y)\n",
    "            while sqrt(sum([coord ** 2 for coord in grad])) > self.max_iter:\n",
    "\n",
    "      \n",
    "\n",
    "        # TODO START: Complete the function\n",
    "        ...\n",
    "        # TODO END\n",
    "\n",
    "    def fit(self, X, y):\n",
    "\n",
    "\n",
    "        N, D = X.shape\n",
    "        #self._initialize_weights(X.shape[1])\n",
    "        # Adding a column of ones at the beginning for the bias term\n",
    "        ones_col = np.ones((N, 1))\n",
    "        X = np.hstack((ones_col, X))\n",
    "        \n",
    "        # Initializing the weights\n",
    "        if self.theta_init is None:\n",
    "            theta_old = np.zeros((D + 1,))\n",
    "        else:\n",
    "            theta_old = self.theta_init\n",
    "\n",
    "        # Initializing the historical weights matrix\n",
    "        # Remember to append this matrix with the weights after every gradient descent iteration\n",
    "        self.hist_theta_ = np.array([theta_old])\n",
    "\n",
    "        # Computing the cost for the initial weights\n",
    "        cost = self.compute_cost(theta_old, X, y)\n",
    "\n",
    "        # Initializing the historical cost array\n",
    "        # Remember to append this array with the cost after every gradient descent iteration\n",
    "        self.hist_cost_ = np.array([cost])\n",
    "        \n",
    "        \n",
    "        \n",
    "        # TODO START: Complete the function\n",
    "        ...\n",
    "        # TODO END\n",
    "\n",
    "    def predict(self, X):\n",
    "\n",
    "    \n",
    "\n",
    "        N = X.shape[0]\n",
    "        X = np.hstack((np.ones((N, 1)), X))\n",
    "        \n",
    "        # TODO START: Complete the function\n",
    "        ...\n",
    "        # TODO END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_convergence(self,\n",
    "                        theta_old,\n",
    "                        theta_new,\n",
    "                        step,\n",
    "                        max_iter,\n",
    "                        iter_below_tol,\n",
    "                        min_num_iter=0):\n",
    "    \"\"\"Checks if training for a model has converged.\"\"\"\n",
    "    has_converged = False\n",
    "\n",
    "    # Check if we have reached the desired loss tolerance.\n",
    "    loss_diff = abs(theta_old - theta_new)\n",
    "    if loss_diff < self.hist_theta_ or abs(\n",
    "        loss_diff / theta_old) < self.hist_cost_:\n",
    "      iter_below_tol += 1\n",
    "    else:\n",
    "      iter_below_tol = 0\n",
    "    if iter_below_tol >= self.loss_chg_iter_below_tol:\n",
    "      # print('Loss value converged.')\n",
    "      has_converged = True\n",
    "\n",
    "    # Make sure that irrespective of the stop criteria, the minimum required\n",
    "    # number of iterations is achieved.\n",
    "    if step < min_num_iter:\n",
    "      has_converged = False\n",
    "    else:\n",
    "      has_converged = True\n",
    "\n",
    "    # Make sure we don't exceed the max allowed number of iterations.\n",
    "    if max_iter is not None and step >= max_iter:\n",
    "      print('Maximum number of iterations reached.')\n",
    "      has_converged = True\n",
    "    return has_converged, iter_below_tol "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "    \n",
    "\n",
    "    def __init__(self, alpha = 0.01, tol=1e-4, max_iter = 100, theta_init = None, penalty = None, lambd = 0):\n",
    "    # store meta-data\n",
    "        self.alpha = alpha\n",
    "        self.theta_init = theta_init\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.penalty = penalty\n",
    "        self.lambd = lambd\n",
    "\n",
    "        self.theta_ = None\n",
    "        self.hist_cost_ = None\n",
    "        self.hist_theta_ = None\n",
    "\n",
    "\n",
    "\n",
    "    def cost(self, theta,X,y):\n",
    "        \n",
    "        m = len(y)\n",
    "        \n",
    "        c = (1/2*m) * np.sum(np.square((X.dot(theta))-y))  \n",
    "    \n",
    "        return c\n",
    "\n",
    "\n",
    "    def compute_gradient(self, X,y,theta):\n",
    "        m = len(y)\n",
    "    \n",
    "        thetas = np.zeros((self.max_iter),2)\n",
    "        costs = np.zeros(self.max_iter)\n",
    "    \n",
    "        for i in range(self.max_iter):\n",
    "            self.theta_ = theta - (1/m)* self.alpha*(X.T.dot((X.dot(theta))-y))\n",
    "            self.hist_theta_ [i,:] = theta.T\n",
    "            self.hist_cost_ = self.cost(theta,X,y)\n",
    "            \n",
    "        \n",
    "\n",
    "    \n",
    "    def check_convergence(self, theta_old, theta_new,): # theta_old[-1], theta\n",
    "        \n",
    "        has_converged = False\n",
    "\n",
    "        for i in range(self.max_iter):\n",
    "            loss_diff = abs(theta_old - theta_new)\n",
    "            while loss_diff > self.tol:\n",
    "                if loss_diff < abs(loss_diff / theta_old): \n",
    "                    print('Loss value converged.')\n",
    "                    has_converged = True\n",
    "\n",
    "            # Make sure we don't exceed the max allowed number of iterations.\n",
    "                if i >= self.max_iter:\n",
    "                    print('Maximum number of iterations reached.')\n",
    "      #  has_converged = True\n",
    "        return has_converged, loss_diff\n",
    "\n",
    "\n",
    "    def fit( self, X, y ) :\n",
    "          \n",
    "        # no_of_training_examples, no_of_features\n",
    "          \n",
    "        self.m, self.n = X.shape\n",
    "          \n",
    "        # weight initialization\n",
    "          \n",
    "        self.W = np.zeros( self.n )\n",
    "          \n",
    "        self.b = 0\n",
    "          \n",
    "        self.X = X\n",
    "          \n",
    "        self.y = y\n",
    "          \n",
    "          \n",
    "        # gradient descent learning\n",
    "                  \n",
    "        for i in range( self.max_iter ) :\n",
    "              \n",
    "            self.compute_gradient(self, X, y, self.hist_theta_[i])\n",
    "              \n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad = approx_gradient(f,x,y)\n",
    "\n",
    "while length(grad) > tolerance:\n",
    "        grad = approx_gradient(f,x,y)\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x -= 0.01 * grad[0]\n",
    "y -= 0.01 * grad[1]\n",
    "\n",
    "c = (1/2*m) * np.sum(np.square((X.dot(theta))-y))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testLinearRegression = LinearRegression()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testLinearRegression.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Adding a biasing constant of value 1 to the features array.\n",
    "X_bias = np.c_[np.ones((len(X),1)),X]\n",
    "# Running Gradient Descent\n",
    "theta,thetas,costs = LinearRegression.gradient_descent(X,y,theta)\n",
    "# printing final values.\n",
    "print('Final Theta 0 value: {:0.3f}\\nFinal Theta 1 value: {:0.3f}'.format(theta[0][0],theta[1][0]))\n",
    "print('Final Cost/MSE(L2 Loss) Value: {:0.3f}'.format(costs[-1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_converged(self, theta_old, theta_new):\n",
    "    converged = False\n",
    "    if np.max(self.force.flatten()) < self.f_tol:\n",
    "        if self.max_iter > 0:\n",
    "            if theta_new - theta_old[-2] <1e-3:\n",
    "                converged = True\n",
    "        return converged "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_convergence(theta_old, theta_new, e_rel):\n",
    "    # Calculate the norm for columns and rows, which can be used for debugging\n",
    "    # Otherwise skip, since it takes extra processing time\n",
    "    new_old = theta_new * theta_old\n",
    "    old2 = theta_old ** 2\n",
    "    norms = [np.sum(new_old), np.sum(old2)]\n",
    "    convergent = norms[0] >= (1 - e_rel ** 2) * norms[1]\n",
    "    return convergent, norms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from math import sqrt, sin, cos, acos, atan2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression() :\n",
    "      \n",
    "    def __init__( self, learning_rate, iterations ) :\n",
    "          \n",
    "        self.learning_rate = learning_rate\n",
    "          \n",
    "        self.iterations = iterations\n",
    "          \n",
    "    # Function for model training\n",
    "              \n",
    "    def fit( self, X, Y ) :\n",
    "          \n",
    "        # no_of_training_examples, no_of_features\n",
    "          \n",
    "        self.m, self.n = X.shape\n",
    "          \n",
    "        # weight initialization\n",
    "          \n",
    "        self.W = np.zeros( self.n )\n",
    "          \n",
    "        self.b = 0\n",
    "          \n",
    "        self.X = X\n",
    "          \n",
    "        self.Y = Y\n",
    "          \n",
    "          \n",
    "        # gradient descent learning\n",
    "                  \n",
    "        for i in range( self.iterations ) :\n",
    "              \n",
    "            self.update_weights()\n",
    "              \n",
    "        return self\n",
    "      \n",
    "    # Helper function to update weights in gradient descent\n",
    "      \n",
    "    def update_weights( self ) :\n",
    "             \n",
    "        Y_pred = self.predict( self.X )\n",
    "          \n",
    "        # calculate gradients  \n",
    "      \n",
    "        dW = - ( 2 * ( self.X.T ).dot( self.Y - Y_pred )  ) / self.m\n",
    "       \n",
    "        db = - 2 * np.sum( self.Y - Y_pred ) / self.m \n",
    "          \n",
    "        # update weights\n",
    "      \n",
    "        self.W = self.W - self.learning_rate * dW\n",
    "      \n",
    "        self.b = self.b - self.learning_rate * db\n",
    "          \n",
    "        return self\n",
    "      \n",
    "    # Hypothetical function  h( x ) \n",
    "      \n",
    "    def predict( self, X ) :\n",
    "      \n",
    "        return X.dot( self.W ) + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Fri Dec 21 18:59:49 2018\n",
    "@author: Nhan Tran\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Importing the dataset\n",
    "dataset = pd.read_csv('./Sample Data/PART 2. REGRESSION - Polynomial Regression - Polynomial_Regression/Polynomial_Regression/Position_Salaries.csv')\n",
    "X = dataset.iloc[:, 1:2].values\n",
    "y = dataset.iloc[:, 2].values\n",
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "\"\"\"\n",
    "# Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_X = StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.transform(X_test)\n",
    "\"\"\"\n",
    "\n",
    "# Fitting Linear Regression to the dataset\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X, y)\n",
    "\n",
    "# Visualizing the Linear Regression results\n",
    "def viz_linear():\n",
    "    plt.scatter(X, y, color='red')\n",
    "    plt.plot(X, lin_reg.predict(X), color='blue')\n",
    "    plt.title('Truth or Bluff (Linear Regression)')\n",
    "    plt.xlabel('Position level')\n",
    "    plt.ylabel('Salary')\n",
    "    plt.show()\n",
    "    return\n",
    "viz_linear()\n",
    "\n",
    "# Fitting Polynomial Regression to the dataset\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly_reg = PolynomialFeatures(degree=4)\n",
    "X_poly = poly_reg.fit_transform(X)\n",
    "pol_reg = LinearRegression()\n",
    "pol_reg.fit(X_poly, y)\n",
    "\n",
    "# Visualizing the Polymonial Regression results\n",
    "def viz_polymonial():\n",
    "    plt.scatter(X, y, color='red')\n",
    "    plt.plot(X, pol_reg.predict(poly_reg.fit_transform(X)), color='blue')\n",
    "    plt.title('Truth or Bluff (Linear Regression)')\n",
    "    plt.xlabel('Position level')\n",
    "    plt.ylabel('Salary')\n",
    "    plt.show()\n",
    "    return\n",
    "viz_polymonial()\n",
    "\n",
    "# Additional feature\n",
    "# Making the plot line (Blue one) more smooth\n",
    "def viz_polymonial_smooth():\n",
    "    X_grid = np.arange(min(X), max(X), 0.1)\n",
    "    X_grid = X_grid.reshape(len(X_grid), 1) #Why do we need to reshape? (https://www.tutorialspoint.com/numpy/numpy_reshape.htm)\n",
    "    # Visualizing the Polymonial Regression results\n",
    "    plt.scatter(X, y, color='red')\n",
    "    plt.plot(X_grid, pol_reg.predict(poly_reg.fit_transform(X_grid)), color='blue')\n",
    "    plt.title('Truth or Bluff (Linear Regression)')\n",
    "    plt.xlabel('Position level')\n",
    "    plt.ylabel('Salary')\n",
    "    plt.show()\n",
    "    return\n",
    "viz_polymonial_smooth()\n",
    "\n",
    "# Predicting a new result with Linear Regression\n",
    "lin_reg.predict([[5.5]])\n",
    "#output should be 249500\n",
    "\n",
    "# Predicting a new result with Polymonial Regression\n",
    "pol_reg.predict(poly_reg.fit_transform([[5.5]]))\n",
    "#output should be 132148.43750003"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('blockchainDev')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1e9c44cc3bc95fb73b0aae724063041b6d6060f5ce93be071123fdfeed5e731e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
