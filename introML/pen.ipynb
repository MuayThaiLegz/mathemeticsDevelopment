{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qs0yamHN-dY0"
      },
      "source": [
        "# **CIS 4190/5190 Fall 2022 - Homework 2**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 250,
      "metadata": {
        "id": "Rb-WLp5Z-cdy"
      },
      "outputs": [],
      "source": [
        "import random \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy.linalg import *\n",
        "np.random.seed(42)  # don't change this line"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfs6aWUhmWJT"
      },
      "source": [
        "# **1. Linear Regression**\n",
        "\n",
        "## **1.1. Linear Regression Implementation [15 pts, autograded]**\n",
        "\n",
        "In this section you will implement linear regression with both L1 and L2 regularization. Your class LinearRegression must implement the following API:\n",
        "\n",
        "* `__init__(alpha, tol, max_iter, theta_init, penalty, lambd)`\n",
        "* `compute_cost(theta, X, y)`\n",
        "* `compute_gradient(theta, X, y)`\n",
        "* `fit(X, y)`\n",
        "* `has_converged(theta_old, theta_new)`\n",
        "* `predict(X)`\n",
        "\n",
        "Note that these methods have already been defined correctly for you in the LinearRegression class. **DO NOT** change the API.\n",
        "\n",
        "### **1.1.1. Cost Function [5 pts]**\n",
        "\n",
        "The `compute_cost` function should compute the cost for a given $\\theta$ vector. The cost is a scalar value given by:\n",
        "\n",
        "$\n",
        "\\mathcal{L}({\\theta}) = \\frac{1}{N}\\sum_{i =1}^N (h_{{\\theta}}({x}_i) - y_i)^2\n",
        "$\n",
        "\n",
        "where\n",
        "\n",
        "> $h_{{\\theta}}({x}_i) = \\theta^Tx_i$\n",
        "\n",
        "Based on the regularisation penalty, you may need to add below regularisation penalty loss to MSE Loss computed previously.\n",
        "\n",
        "L1 Regularisation Loss:\n",
        ">$\n",
        "\\mathcal{L_1}({\\theta}) = \\mathcal{L}({\\theta}) + \\lambda\\sum_{j = 1}^D  |{\\theta}_j|\n",
        "$\n",
        "\n",
        "L2 Regularisation Loss:\n",
        ">$\n",
        "\\mathcal{L_2}({\\theta}) = \\mathcal{L}({\\theta}) + \\lambda\\sum_{j = 1}^D  {\\theta}_j^2 \n",
        "$\n",
        "\n",
        "$N$ is the number of training samples and $D$ is the number of features (excluding the intercept term). $\\theta$ is a $D + 1$ dimensional vector, with the first element being the intercept term. Note that we do not include the intercept in the regularization terms.\n",
        "\n",
        "---\n",
        "\n",
        "### **1.1.2. Gradient of the Cost Function [5 pts]**\n",
        "\n",
        "The `compute_gradient` function should compute the gradient of the cost function at a given $\\theta$.\n",
        "\n",
        "---\n",
        "\n",
        "### **1.1.3. Convergence Check [1 pt]**\n",
        "\n",
        "The `has_converged` function should return whether gradient descent algorithm has converged or not. Refer 1.1.4 for convergence condition.\n",
        " \n",
        "---\n",
        "\n",
        "### **1.1.4. Training [3 pts]**\n",
        "\n",
        "The `fit` method should train the model via gradient descent, relying on the cost and gradient functions. The trained weights/coefficients must be stored as `theta_`. The weights and the corresponding cost after every gradient descent iteration must be stored in `hist_theta_` and `hist_cost_` respectively.\n",
        "\n",
        "* The gradient descent stops or converges when $\\theta$ stops changing or changes negligibly between consecutive iterations, i.e., when \n",
        "$\\| {\\theta}_\\mathit{new} -  {\\theta}_\\mathit{old} \\|_2 \\leq \\epsilon$, \n",
        "for some small $\\epsilon$ (e.g., $\\epsilon$ = 1E-4). $\\epsilon$ is stored as `tol` (short for tolerance). \n",
        "\n",
        "* To ensure that the function terminates, we should set a maximum limit for the number of gradient descent iterations irrespective of whether $\\theta$ converges or not. The limit is stored as `max_iter`.\n",
        "\n",
        "* `alpha` is the learning rate of the gradient descent algorithm.\n",
        "\n",
        "---\n",
        "\n",
        "### **1.1.5. Predict [1 pt]**\n",
        "\n",
        "The `predict` function should predict the output for the data points in a given input data matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 282,
      "metadata": {
        "id": "x_iD4A-TmjKe"
      },
      "outputs": [],
      "source": [
        "class LinearRegression:\n",
        "\n",
        "    \"\"\"\n",
        "    Linear Regression\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    alpha: float, default=0.01\n",
        "        Learning rate\n",
        "    tol : float, default=0.0001\n",
        "        Tolerance for stopping criteria\n",
        "    max_iter : int, default=10000\n",
        "        Maximum number of iterations of gradient descent\n",
        "    theta_init: None (or) numpy.ndarray of shape (D + 1,)\n",
        "        The initial weights; if None, all weights will be zero by default\n",
        "    penalty : string, default = None\n",
        "        The type of regularization. The other acceptable options are l1 and l2\n",
        "    lambd : float, default = 1.0\n",
        "        The parameter regularisation constant (i.e. lambda)\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    theta_ : numpy.ndarray of shape (D + 1,)\n",
        "        The value of the coefficients after gradient descent has converged\n",
        "        or the number of iterations hit the maximum limit\n",
        "    hist_theta_ : numpy.ndarray of shape (num_iter, D + 1) where num_iter is the number of gradient descent iterations\n",
        "        Stores theta_ after every gradient descent iteration\n",
        "    hist_cost_ : numpy.ndarray of shape (num_iter,) where num_iter is the number of gradient descent iterations\n",
        "        Stores cost after every gradient descent iteration\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, alpha = 0.01, tol=1e-4, max_iter = 100, theta_init = None, penalty = None, lambd = 0):\n",
        "        \n",
        "        # store meta-data\n",
        "        self.alpha = alpha\n",
        "        self.theta_init = theta_init\n",
        "        self.max_iter = max_iter\n",
        "        self.tol = tol\n",
        "        self.penalty = penalty\n",
        "        self.lambd = lambd\n",
        "\n",
        "        self.theta_ = None\n",
        "        self.hist_cost_ = None\n",
        "        self.hist_theta_ = None\n",
        "    \n",
        "    def compute_cost(self, theta, X, y):\n",
        "    \n",
        "        \"\"\"\n",
        "        Compute the cost/objective function.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        theta: numpy.ndarray of shape (D + 1,)\n",
        "            The coefficients\n",
        "        X: numpy.ndarray of shape (N, D + 1)\n",
        "            The features matrix\n",
        "        y: numpy.ndarray of shape (N,)\n",
        "            The target variable array\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        cost: float\n",
        "            The cost as a scalar value\n",
        "        \"\"\"\n",
        "        \n",
        "        # TODO STARTS: Complete the function (should account for three cases - no penalty, l1 penalty, and l2 penalty)\n",
        "        #self.m, self.n = X.shape\n",
        "\n",
        "        n = len(y)\n",
        "        D = len(theta)\n",
        "        predictions = X.dot(theta)\n",
        "        totalCost = (1/2*n) * np.sum(np.square(predictions-y))  \n",
        "        if self.penalty == \"l1\":\n",
        "          sum = 0\n",
        "          for i in range(1, D):\n",
        "            sum = sum + np.abs(theta[i]) \n",
        "          totalCost = totalCost + self.lambd * sum\n",
        "        elif self.penalty == \"l2\":\n",
        "          sum = 0\n",
        "          for i in range(1, D):\n",
        "            sum = sum + np.square(theta[i]) \n",
        "          totalCost = totalCost + self.lambd * sum\n",
        "        return totalCost\n",
        "        # TODO ENDS\n",
        "\n",
        "    def gradientDescent(self, X, y, theta):\n",
        "        n = len(y)\n",
        "        #d = len(theta)\n",
        "        self.hist_cost_ = np.zeros(self.max_iter)\n",
        "        self.hist_theta_ = np.zeros((self.max_iter,2))  \n",
        "        theta  = np.zeros((self.max_iter,2))\n",
        "        X = np.zeros((self.max_iter,2))\n",
        "\n",
        "        for it in range(self.max_iter):\n",
        "            \n",
        "            yhat = np.dot(X, theta)\n",
        "            \n",
        "            self.theta_ = theta -(1/n)*self.alpha*( X.T.dot((yhat - y)))\n",
        "            self.hist_theta_[it,:] =theta.T \n",
        "            \n",
        "            self.hist_cost_ = self.compute_cost(X, y, theta), theta\n",
        "            \n",
        "        return theta\n",
        "        # TODO ENDS\n",
        "\n",
        "    def has_converged(self, theta_old, theta_new):\n",
        "\n",
        "        \"\"\"\n",
        "        Return whether gradient descent has converged.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        theta_old: numpy.ndarray of shape (D + 1,)\n",
        "            The weights prior to the update by gradient descent\n",
        "        theta_new: numpy.ndarray of shape (D + 1,)\n",
        "            The weights after the update by gradient descent\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        converged: bool\n",
        "            Whether gradient descent converged or not\n",
        "        \"\"\"\n",
        "\n",
        "        # TODO START: Complete the function\n",
        "        ...\n",
        "        # TODO END\n",
        "\n",
        "    def fit(self, X, y):\n",
        "\n",
        "        \"\"\"\n",
        "        Compute the coefficients using gradient descent and store them as theta_.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X: numpy.ndarray of shape (N, D)\n",
        "            The features matrix\n",
        "        y: numpy.ndarray of shape (N,)\n",
        "            The target variable array\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        Nothing\n",
        "        \"\"\"\n",
        "\n",
        "        N, D = X.shape\n",
        "\n",
        "        # Adding a column of ones at the beginning for the bias term\n",
        "        ones_col = np.ones((N, 1))\n",
        "        X = np.hstack((ones_col, X))\n",
        "        \n",
        "        # Initializing the weights\n",
        "        if self.theta_init is None:\n",
        "            theta_old = np.zeros((D + 1,))\n",
        "        else:\n",
        "            theta_old = self.theta_init\n",
        "\n",
        "        # Initializing the historical weights matrix\n",
        "        # Remember to append this matrix with the weights after every gradient descent iteration\n",
        "        self.hist_theta_ = np.array([theta_old])\n",
        "\n",
        "        # Computing the cost for the initial weights\n",
        "        cost = self.compute_cost(theta_old, X, y)\n",
        "\n",
        "        # Initializing the historical cost array\n",
        "        # Remember to append this array with the cost after every gradient descent iteration\n",
        "        self.hist_cost_ = np.array([cost])\n",
        "        \n",
        "        # TODO START: Complete the function\n",
        "        ...\n",
        "        # TODO END\n",
        "\n",
        "    def predict(self, X):\n",
        "\n",
        "        \"\"\"\n",
        "        Predict the target variable values for the data points in X.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X: numpy.ndarray of shape (N, D)\n",
        "            The features matrix\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        y_hat: numpy.ndarray of shape (N,)\n",
        "            The predicted target variables values for the data points in X\n",
        "        \"\"\"\n",
        "\n",
        "        N = X.shape[0]\n",
        "        X = np.hstack((np.ones((N, 1)), X))\n",
        "        \n",
        "        # TODO START: Complete the function\n",
        "        ...\n",
        "        # TODO END"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 283,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0gOdODTWQQX0",
        "outputId": "18ff3106-4170-4bd2-a848-62f604275ca0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4.881828654157735\n",
            "4.943004295157735\n",
            "4.919253244675343\n"
          ]
        }
      ],
      "source": [
        "def test_lin_reg_compute_cost(StudentLinearRegression):\n",
        "    \n",
        "    test_case_theta = np.array([ 1.62434536, -0.61175641])\n",
        "    test_case_X = np.array([[ 1.62434536, -0.61175641],\n",
        "                            [-0.52817175, -1.07296862],\n",
        "                            [ 0.86540763, -2.3015387 ],\n",
        "                            [ 1.74481176, -0.7612069 ],\n",
        "                            [ 0.3190391,  -0.24937038]])\n",
        "    test_case_y = np.array([1, 1, 0, 0, 1])\n",
        "\n",
        "    student_lr_reg = StudentLinearRegression()\n",
        "    student_ans = student_lr_reg.compute_cost(test_case_theta, test_case_X, test_case_y)\n",
        "    print(student_ans)\n",
        "    required_ans = 4.881828654157736\n",
        "    \n",
        "    assert np.abs(student_ans - required_ans) <= 1e-2\n",
        "\n",
        "    student_lr_reg = StudentLinearRegression(penalty=\"l1\", lambd=0.1)\n",
        "    student_ans = student_lr_reg.compute_cost(test_case_theta, test_case_X, test_case_y)\n",
        "    required_ans = 4.94300429515773\n",
        "    print(student_ans)\n",
        "    assert np.abs(student_ans - required_ans) <= 1e-2\n",
        "\n",
        "    student_lr_reg = StudentLinearRegression(penalty=\"l2\", lambd=0.1)\n",
        "    student_ans = student_lr_reg.compute_cost(test_case_theta, test_case_X, test_case_y)\n",
        "    required_ans = 4.919253244675344\n",
        "    print(student_ans)\n",
        "    assert np.abs(student_ans - required_ans) <= 1e-2\n",
        "    \n",
        "test_lin_reg_compute_cost(LinearRegression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ct-hUcbC9Zp1",
        "outputId": "67e5f561-9a34-45ba-cbd7-823bc3a0759d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 285,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "id": "pysdW3awRLl1",
        "outputId": "dbe295dd-cb50-4dcc-b811-ffe82fafdd9a"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "shapes (100,2) and (100,2) not aligned: 2 (dim 1) != 100 (dim 0)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\JBarr\\OneDrive\\Documents\\FinTech\\career_services\\TutoringSessions\\introML\\pen.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 28>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/JBarr/OneDrive/Documents/FinTech/career_services/TutoringSessions/introML/pen.ipynb#X21sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     \u001b[39massert\u001b[39;00m np\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39mnorm(student_ans \u001b[39m-\u001b[39m required_ans) \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1e-2\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/JBarr/OneDrive/Documents/FinTech/career_services/TutoringSessions/introML/pen.ipynb#X21sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39makljhfa\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/JBarr/OneDrive/Documents/FinTech/career_services/TutoringSessions/introML/pen.ipynb#X21sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m test_lin_reg_compute_gradient(LinearRegression)\n",
            "\u001b[1;32mc:\\Users\\JBarr\\OneDrive\\Documents\\FinTech\\career_services\\TutoringSessions\\introML\\pen.ipynb Cell 10\u001b[0m in \u001b[0;36mtest_lin_reg_compute_gradient\u001b[1;34m(StudentLinearRegression)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/JBarr/OneDrive/Documents/FinTech/career_services/TutoringSessions/introML/pen.ipynb#X21sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m test_case_y \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/JBarr/OneDrive/Documents/FinTech/career_services/TutoringSessions/introML/pen.ipynb#X21sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m student_lr_reg \u001b[39m=\u001b[39m StudentLinearRegression()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/JBarr/OneDrive/Documents/FinTech/career_services/TutoringSessions/introML/pen.ipynb#X21sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m student_ans \u001b[39m=\u001b[39m student_lr_reg\u001b[39m.\u001b[39;49mgradientDescent(test_case_theta, test_case_X, test_case_y)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/JBarr/OneDrive/Documents/FinTech/career_services/TutoringSessions/introML/pen.ipynb#X21sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m required_ans \u001b[39m=\u001b[39m [ \u001b[39m4.79663712\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m3.53908485\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/JBarr/OneDrive/Documents/FinTech/career_services/TutoringSessions/introML/pen.ipynb#X21sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mprint\u001b[39m(student_ans)\n",
            "\u001b[1;32mc:\\Users\\JBarr\\OneDrive\\Documents\\FinTech\\career_services\\TutoringSessions\\introML\\pen.ipynb Cell 10\u001b[0m in \u001b[0;36mLinearRegression.gradientDescent\u001b[1;34m(self, X, y, theta)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/JBarr/OneDrive/Documents/FinTech/career_services/TutoringSessions/introML/pen.ipynb#X21sZmlsZQ%3D%3D?line=90'>91</a>\u001b[0m X \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros((\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_iter,\u001b[39m2\u001b[39m))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/JBarr/OneDrive/Documents/FinTech/career_services/TutoringSessions/introML/pen.ipynb#X21sZmlsZQ%3D%3D?line=92'>93</a>\u001b[0m \u001b[39mfor\u001b[39;00m it \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_iter):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/JBarr/OneDrive/Documents/FinTech/career_services/TutoringSessions/introML/pen.ipynb#X21sZmlsZQ%3D%3D?line=94'>95</a>\u001b[0m     yhat \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mdot(X, theta)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/JBarr/OneDrive/Documents/FinTech/career_services/TutoringSessions/introML/pen.ipynb#X21sZmlsZQ%3D%3D?line=96'>97</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtheta_ \u001b[39m=\u001b[39m theta \u001b[39m-\u001b[39m(\u001b[39m1\u001b[39m\u001b[39m/\u001b[39mn)\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39malpha\u001b[39m*\u001b[39m( X\u001b[39m.\u001b[39mT\u001b[39m.\u001b[39mdot((yhat \u001b[39m-\u001b[39m y)))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/JBarr/OneDrive/Documents/FinTech/career_services/TutoringSessions/introML/pen.ipynb#X21sZmlsZQ%3D%3D?line=97'>98</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhist_theta_[it,:] \u001b[39m=\u001b[39mtheta\u001b[39m.\u001b[39mT \n",
            "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[1;31mValueError\u001b[0m: shapes (100,2) and (100,2) not aligned: 2 (dim 1) != 100 (dim 0)"
          ]
        }
      ],
      "source": [
        "def test_lin_reg_compute_gradient(StudentLinearRegression):\n",
        "    \n",
        "    test_case_theta = np.array([ 1.62434536, -0.61175641])\n",
        "    test_case_X = np.array([[ 1.62434536, -0.61175641],\n",
        "                            [-0.52817175, -1.07296862],\n",
        "                            [ 0.86540763, -2.3015387 ],\n",
        "                            [ 1.74481176, -0.7612069 ],\n",
        "                            [ 0.3190391,  -0.24937038]])\n",
        "    test_case_y = np.array([1, 1, 0, 0, 1])\n",
        "\n",
        "    student_lr_reg = StudentLinearRegression()\n",
        "    student_ans = student_lr_reg.gradientDescent(test_case_theta, test_case_X, test_case_y)\n",
        "    required_ans = [ 4.79663712, -3.53908485]\n",
        "    print(student_ans)\n",
        "    assert np.linalg.norm(student_ans - required_ans) <= 1e-2\n",
        "\n",
        "    student_lr_reg = StudentLinearRegression(penalty=\"l1\", lambd=0.1)\n",
        "    student_ans = student_lr_reg.gradientDescent(test_case_theta, test_case_X, test_case_y)\n",
        "    required_ans = [ 4.79663712, -3.63908485]\n",
        "\n",
        "    assert np.linalg.norm(student_ans - required_ans) <= 1e-2\n",
        "\n",
        "    print('akljhfa')\n",
        "    student_lr_reg = StudentLinearRegression(penalty=\"l2\", lambd=0.1)\n",
        "    student_ans = student_lr_reg.gradientDescent(test_case_theta, test_case_X, test_case_y)\n",
        "    required_ans = [ 4.79663712, -3.66143613]\n",
        "    \n",
        "    \n",
        "    assert np.linalg.norm(student_ans - required_ans) <= 1e-2\n",
        "    print('akljhfa')\n",
        "\n",
        "test_lin_reg_compute_gradient(LinearRegression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VptfbtsMAEVB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 286,
      "metadata": {
        "id": "naEgZPDXQ5_U"
      },
      "outputs": [
        {
          "ename": "AssertionError",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\JBarr\\OneDrive\\Documents\\FinTech\\career_services\\TutoringSessions\\introML\\pen.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/JBarr/OneDrive/Documents/FinTech/career_services/TutoringSessions/introML/pen.ipynb#X23sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     required_ans \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/JBarr/OneDrive/Documents/FinTech/career_services/TutoringSessions/introML/pen.ipynb#X23sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39massert\u001b[39;00m student_ans \u001b[39m==\u001b[39m required_ans\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/JBarr/OneDrive/Documents/FinTech/career_services/TutoringSessions/introML/pen.ipynb#X23sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m test_lin_reg_has_converged(LinearRegression)\n",
            "\u001b[1;32mc:\\Users\\JBarr\\OneDrive\\Documents\\FinTech\\career_services\\TutoringSessions\\introML\\pen.ipynb Cell 12\u001b[0m in \u001b[0;36mtest_lin_reg_has_converged\u001b[1;34m(StudentLinearRegression)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/JBarr/OneDrive/Documents/FinTech/career_services/TutoringSessions/introML/pen.ipynb#X23sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m student_ans \u001b[39m=\u001b[39m student_lr_reg\u001b[39m.\u001b[39mhas_converged(test_case_theta_old, test_case_theta_new)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/JBarr/OneDrive/Documents/FinTech/career_services/TutoringSessions/introML/pen.ipynb#X23sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m required_ans \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/JBarr/OneDrive/Documents/FinTech/career_services/TutoringSessions/introML/pen.ipynb#X23sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39massert\u001b[39;00m student_ans \u001b[39m==\u001b[39m required_ans\n",
            "\u001b[1;31mAssertionError\u001b[0m: "
          ]
        }
      ],
      "source": [
        "def test_lin_reg_has_converged(StudentLinearRegression):\n",
        "\n",
        "    student_lr_reg = StudentLinearRegression()\n",
        "    test_case_theta_old = np.array([ 1.62434536, -0.61175641])\n",
        "    test_case_theta_new = np.array([1.624345, -0.611756])\n",
        "    student_ans = student_lr_reg.has_converged(test_case_theta_old, test_case_theta_new)\n",
        "    required_ans = True\n",
        "    \n",
        "    assert student_ans == required_ans\n",
        "\n",
        "\n",
        "test_lin_reg_has_converged(LinearRegression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7rx213_3_gmj"
      },
      "outputs": [],
      "source": [
        "# PennGrader Grading Cell\n",
        "if NOTEBOOK:\n",
        "    grader.grade(test_case_id = 'test_lin_reg_has_converged', answer = LinearRegression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NF9jGYOVSrC2"
      },
      "outputs": [],
      "source": [
        "def test_lin_reg_fit(StudentLinearRegression):\n",
        "    \n",
        "    student_lr_reg = StudentLinearRegression(max_iter=5)\n",
        "    test_case_X = np.array([[ 1.62434536, -0.61175641],\n",
        "                            [-0.52817175, -1.07296862],\n",
        "                            [ 0.86540763, -2.3015387 ],\n",
        "                            [ 1.74481176, -0.7612069 ],\n",
        "                            [ 0.3190391,  -0.24937038]])\n",
        "    test_case_y = np.array([1, 1, 0, 0, 1])\n",
        "    student_lr_reg.fit(test_case_X, test_case_y)\n",
        "    student_ans = student_lr_reg.hist_theta_\n",
        "    required_ans = np.array([[ 0.        ,  0.        ,  0.        ],\n",
        "       [ 0.012     ,  0.00566085, -0.00773638],\n",
        "       [ 0.02351422,  0.01085581, -0.01491529],\n",
        "       [ 0.03457102,  0.01561393, -0.0215702 ],\n",
        "       [ 0.04519706,  0.01996249, -0.02773259],\n",
        "       [ 0.05541739,  0.02392713, -0.03343205]])\n",
        "    assert np.linalg.norm(student_ans - required_ans) <= 1e-2\n",
        "\n",
        "if NOTEBOOK:\n",
        "    test_lin_reg_fit(LinearRegression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sewbA8dC5Fdm"
      },
      "outputs": [],
      "source": [
        "# PennGrader Grading Cell\n",
        "if NOTEBOOK:\n",
        "    grader.grade(test_case_id = 'test_lin_reg_fit', answer = grader_serialize(LinearRegression))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQQN2ky4UYxx"
      },
      "outputs": [],
      "source": [
        "def test_lin_reg_predict(StudentLinearRegression):\n",
        "\n",
        "    student_lr_reg = StudentLinearRegression(max_iter=5)\n",
        "    np.random.seed(1)\n",
        "    test_case_X = np.random.randn(50, 2)\n",
        "    test_case_y = np.random.randint(0, 2, 50)\n",
        "    student_lr_reg.fit(test_case_X, test_case_y)\n",
        "    student_ans = student_lr_reg.predict(test_case_X)\n",
        "    required_ans = np.array([0.04739416, 0.02735934, 0.02140787, 0.04634383, 0.04320043,\n",
        "       0.02836861, 0.03726417, 0.03808224, 0.03214353, 0.05166998,\n",
        "       0.05102933, 0.05639199, 0.0416892 , 0.03175554, 0.04895695,\n",
        "       0.03465034, 0.02912364, 0.03954521, 0.0396391 , 0.06440433,\n",
        "       0.03189335, 0.06016748, 0.03661307, 0.07146111, 0.05261461,\n",
        "       0.04180017, 0.03223834, 0.0500466 , 0.06128615, 0.05703506,\n",
        "       0.05467262, 0.04388664, 0.04648138, 0.07052753, 0.04140456,\n",
        "       0.02830984, 0.05608863, 0.0212115 , 0.05238969, 0.05514024,\n",
        "       0.04020117, 0.05048966, 0.04696158, 0.04438422, 0.05897309,\n",
        "       0.05443805, 0.03375689, 0.04794345, 0.04242038, 0.04869202])\n",
        "    \n",
        "    assert np.mean(np.abs(student_ans - required_ans)) <= 1e-2\n",
        "\n",
        "if NOTEBOOK:\n",
        "    test_lin_reg_predict(LinearRegression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WKhfPR6uUJm9"
      },
      "outputs": [],
      "source": [
        "# PennGrader Grading Cell\n",
        "if NOTEBOOK:\n",
        "    grader.grade(test_case_id = 'test_lin_reg_predict', answer = grader_serialize(LinearRegression))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGlB3lojik_m"
      },
      "source": [
        "## **1.2. Synthetic dataset [Ungraded]**\n",
        "\n",
        "In this section we will first create some synthetic data on which we will run your linear regression implementation. We are creating 100 datapoints around the function y = mx + b, introducing Gaussian noise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rqp2jLGTiJQo"
      },
      "outputs": [],
      "source": [
        "# Don't modify this cell\n",
        "if NOTEBOOK:\n",
        "    num_samples = 100\n",
        "\n",
        "    np.random.seed(1)\n",
        "    noise = np.random.randn(num_samples, 1)\n",
        "    X = np.random.randn(num_samples, 1)\n",
        "\n",
        "    y_ideal = 11*X + 5\n",
        "    y_real = (11*X + 5) + noise\n",
        "\n",
        "    plt.plot(X, y_real, 'ro')\n",
        "    plt.plot(X, y_ideal, 'b')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2G9qQh_uiXQt"
      },
      "source": [
        "We see that this data is clearly regressable with a line, which, ideally, would be 11x + 5\n",
        "\n",
        "Train a linear regression model using gradient descent, you should see that training loss goes down with the number of iterations and obtain a theta that converges to a value very close to [b, m], which in this case, for 11x + 5, would be theta = [5, 11]\n",
        "\n",
        "Also, notice the effect of the type of regularisation on the theta obtained (after convergence) as well as the testing MSE loss. Do they make sense, given what was discussed in class? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYzOlitsiNCo"
      },
      "outputs": [],
      "source": [
        "import sklearn\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def test_synthetic_data_sgd(X, y, n_iter = 2000, penalty=None, lambd=0):\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=37)\n",
        "  # Given that we want to get theta as the weights of the linear equation, we won't \n",
        "  # standardize in this section\n",
        "\n",
        "  alpha = 0.03  # Learning Rate\n",
        "\n",
        "  # # Train the model\n",
        "  lr_model = LinearRegression(alpha = alpha, tol=1e-4, max_iter = n_iter, penalty=penalty, lambd=lambd)\n",
        "  lr_model.fit(X_train,y_train[:, 0])\n",
        "  y_predict = lr_model.predict(X_test)\n",
        "  loss = sklearn.metrics.mean_squared_error(y_predict, y_test)\n",
        "  print(\" Theta: {} \\n Norm of Theta: {} \\n Testing MSELoss: {}\".format(lr_model.theta_, np.linalg.norm(lr_model.theta_, ord=2), loss))\n",
        "\n",
        "  loss_history = lr_model.hist_cost_\n",
        "  plt.plot(range(len(loss_history)), loss_history)\n",
        "  plt.title(\"OLS Training Loss\")\n",
        "  plt.xlabel(\"iteration\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "  if penalty == \"l1\":\n",
        "    plt.title(\"L1 Regularised Training Loss\")\n",
        "  elif penalty == \"l2\":\n",
        "    plt.title(\"L2 Regularised Training Loss\")\n",
        "  plt.show()\n",
        "\n",
        "if NOTEBOOK:\n",
        "    test_synthetic_data_sgd(X, y_ideal, 500)\n",
        "    test_synthetic_data_sgd(X, y_ideal, 500, \"l1\", 0.02)\n",
        "    test_synthetic_data_sgd(X, y_ideal, 500, \"l2\", 0.02)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qUcFLi6tjL_"
      },
      "source": [
        "## **1.3. Effect of polynomial degree on training and validation error [5 pts, manually graded] [optional for CIS 4190]**\n",
        "\n",
        "Now, we consider a dataset that was generated using some higher degree polynomial function of the input variable. We do not know the degree of the underlying polynomial. Let us assume it to be an unknown value \"p\" and try to estimate it.\n",
        "\n",
        "Polynomial regression hypothesis for one input variable  or feature (x) can be written as:\n",
        "> $y = w_0 + w_1x + w_2x^2 + ... + w_px^p $\n",
        "\n",
        "If you observe carefully, this can still be solved as a linear regression, where, instead of just 2 weights, we have p+1 weights, and the new features are higher order terms of the original feature. Using this idea, in this section, we will investigate how changing the assumed polynomial degree \"p\" in our model affects the training and validation error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UcpXD4pTarFC"
      },
      "outputs": [],
      "source": [
        "\n",
        "    \n",
        "poly_reg_df = pd.read_csv('cis519_hw2_poly_reg.csv')  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = np.array([258.0, 270.0, 294.0, \n",
        "              320.0, 342.0, 368.0, \n",
        "              396.0, 446.0, 480.0])\n",
        "\n",
        "y = np.array([236.4, 234.4, 252.8, \n",
        "              298.6, 314.2, 342.2, \n",
        "              360.8, 368.0, 391.2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {},
      "outputs": [],
      "source": [
        "xaffasd = np.arange(0, 30)\n",
        "yfasf = [3, 4, 5, 7, 10, 8, 9, 10, 10, 23, 27, 44, 50, 63, 67, 60, 62, 70, 75, 88, 81, 87, 95, 100, 108, 135, 151, 160, 169, 179]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sum_squared_error(f,data):\n",
        "    squared_errors = [(f(x) - y)**2 for (x,y) in data]\n",
        "    return sum(squared_errors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "id": "qJMhCZ6SuepA"
      },
      "outputs": [],
      "source": [
        "import sklearn\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression as LinearRegressionSklearn\n",
        "\n",
        "def polynomial_regression(poly_reg_df, degrees):\n",
        "    xaffasd = np.arange(0, 30)\n",
        "    yfasf = [3, 4, 5, 7, 10, 8, 9, 10, 10, 23, 27, 44, 50, 63, 67, 60, 62, 70, 75, 88, 81, 87, 95, 100, 108, 135, 151, 160, 169, 179]\n",
        "    \"\"\"\n",
        "    Runs polynomial regression on the dataset 'poly_reg_df' for all the powers in 'degrees'\n",
        "    \"\"\"\n",
        "\n",
        "    loss_train_list = []\n",
        "    loss_test_list = []\n",
        "\n",
        "    X_base = xaffasd\n",
        "    y = yfasf\n",
        "\n",
        "    for d in degrees:\n",
        "\n",
        "        # TODO START: Complete the function:\n",
        "        # 1. Transform the base feature X_base into its polynomial features of degree 'd' using PolynomialFeatures\n",
        "        # Set include_bias to be False\n",
        "        PolynomialFea = PolynomialFeatures(degree=d, include_bias=False)\n",
        "        X_base_Poly = PolynomialFea.fit_transform(X_base)\n",
        "\n",
        "        \n",
        "        # 2. Preprocessing and splitting into train/test (70-30 ratio and random_state as 42)\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.30, random_state=42)\n",
        "        \n",
        "        # 3. Scale X_train and X_test appropriately \n",
        "        scaLER = StandardScaler(X)\n",
        "        X_train_Poly = scaLER.fit_transform(X_train)\n",
        "        X_test_Poly = scaLER.fit_transform(X_test)\n",
        "\n",
        "        \n",
        "        # 4. Use scikit-learn's LinearRegression (imported as LinearRegressionSklearn for you) to \n",
        "        # fit a linear model between the scaled version of X_train and y_train\n",
        "        lr_model = LinearRegressionSklearn()\n",
        "        lr_model.fit(X_train_Poly, y_train)\n",
        "        \n",
        "        # 5. Obtain predictions of the model on train and test data\n",
        "        per = lr_model.predict(X_train_Poly, y_train)\n",
        "        \n",
        "        # 6. Compute the mean squared error and store it in loss_train and loss_test\n",
        "        loss_train = sklearn.metrics.mean_squared_error(X_train_Poly, y_train)\n",
        "        loss_test = sklearn.metrics.mean_squared_error(X_test_Poly, y_test)\n",
        "\n",
        "        # 7. Append loss_train to loss_train_list and loss_test to loss_test_list\n",
        "        loss_train_list.append(loss_train)\n",
        "        loss_test_list.append(loss_test)\n",
        "\n",
        "    return loss_train_list, loss_test_list\n",
        "    # TODO END"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "id": "aDiIc_XjuBqk"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Expected 2D array, got 1D array instead:\narray=[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n 24 25 26 27 28 29].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\JBarr\\OneDrive\\Documents\\FinTech\\career_services\\TutoringSessions\\introML\\pen.ipynb Cell 28\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/JBarr/OneDrive/Documents/FinTech/career_services/TutoringSessions/introML/pen.ipynb#X42sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m degrees \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marange(\u001b[39m1\u001b[39m, \u001b[39m9\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/JBarr/OneDrive/Documents/FinTech/career_services/TutoringSessions/introML/pen.ipynb#X42sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m loss_train_list, loss_test_list \u001b[39m=\u001b[39m polynomial_regression(poly_reg_df, degrees)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/JBarr/OneDrive/Documents/FinTech/career_services/TutoringSessions/introML/pen.ipynb#X42sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m# TODO START:\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/JBarr/OneDrive/Documents/FinTech/career_services/TutoringSessions/introML/pen.ipynb#X42sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39m# Plot the polynomial degrees (x-axis) against loss_train_list (y-axis) and loss_test_list (y-axis) in a single plot, with different colors.\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/JBarr/OneDrive/Documents/FinTech/career_services/TutoringSessions/introML/pen.ipynb#X42sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39m# Make sure to include x and y axis labels, legend as well as the title\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/JBarr/OneDrive/Documents/FinTech/career_services/TutoringSessions/introML/pen.ipynb#X42sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39m# fit linear features\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/JBarr/OneDrive/Documents/FinTech/career_services/TutoringSessions/introML/pen.ipynb#X42sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m plt\u001b[39m.\u001b[39mscatter(X, y, label\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTraining points\u001b[39m\u001b[39m'\u001b[39m)\n",
            "\u001b[1;32mc:\\Users\\JBarr\\OneDrive\\Documents\\FinTech\\career_services\\TutoringSessions\\introML\\pen.ipynb Cell 28\u001b[0m in \u001b[0;36mpolynomial_regression\u001b[1;34m(poly_reg_df, degrees)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/JBarr/OneDrive/Documents/FinTech/career_services/TutoringSessions/introML/pen.ipynb#X42sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m degrees:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/JBarr/OneDrive/Documents/FinTech/career_services/TutoringSessions/introML/pen.ipynb#X42sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/JBarr/OneDrive/Documents/FinTech/career_services/TutoringSessions/introML/pen.ipynb#X42sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     \u001b[39m# TODO START: Complete the function:\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/JBarr/OneDrive/Documents/FinTech/career_services/TutoringSessions/introML/pen.ipynb#X42sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     \u001b[39m# 1. Transform the base feature X_base into its polynomial features of degree 'd' using PolynomialFeatures\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/JBarr/OneDrive/Documents/FinTech/career_services/TutoringSessions/introML/pen.ipynb#X42sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     \u001b[39m# Set include_bias to be False\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/JBarr/OneDrive/Documents/FinTech/career_services/TutoringSessions/introML/pen.ipynb#X42sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     PolynomialFea \u001b[39m=\u001b[39m PolynomialFeatures(degree\u001b[39m=\u001b[39md, include_bias\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/JBarr/OneDrive/Documents/FinTech/career_services/TutoringSessions/introML/pen.ipynb#X42sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     X_base_Poly \u001b[39m=\u001b[39m PolynomialFea\u001b[39m.\u001b[39;49mfit_transform(X_base)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/JBarr/OneDrive/Documents/FinTech/career_services/TutoringSessions/introML/pen.ipynb#X42sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     \u001b[39m# 2. Preprocessing and splitting into train/test (70-30 ratio and random_state as 42)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/JBarr/OneDrive/Documents/FinTech/career_services/TutoringSessions/introML/pen.ipynb#X42sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     X_train, X_test, y_train, y_test \u001b[39m=\u001b[39m train_test_split(X, y, test_size\u001b[39m=\u001b[39m\u001b[39m.30\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\JBarr\\anaconda3\\envs\\blockchainDev\\lib\\site-packages\\sklearn\\base.py:867\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    863\u001b[0m \u001b[39m# non-optimized default implementation; override when a better\u001b[39;00m\n\u001b[0;32m    864\u001b[0m \u001b[39m# method is possible for a given clustering algorithm\u001b[39;00m\n\u001b[0;32m    865\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    866\u001b[0m     \u001b[39m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[1;32m--> 867\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit(X, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\u001b[39m.\u001b[39mtransform(X)\n\u001b[0;32m    868\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    869\u001b[0m     \u001b[39m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n",
            "File \u001b[1;32mc:\\Users\\JBarr\\anaconda3\\envs\\blockchainDev\\lib\\site-packages\\sklearn\\preprocessing\\_polynomial.py:287\u001b[0m, in \u001b[0;36mPolynomialFeatures.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    270\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    271\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    272\u001b[0m \u001b[39m    Compute number of output features.\u001b[39;00m\n\u001b[0;32m    273\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[39m        Fitted transformer.\u001b[39;00m\n\u001b[0;32m    286\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 287\u001b[0m     _, n_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(X, accept_sparse\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\u001b[39m.\u001b[39mshape\n\u001b[0;32m    289\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdegree, numbers\u001b[39m.\u001b[39mIntegral):\n\u001b[0;32m    290\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdegree \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n",
            "File \u001b[1;32mc:\\Users\\JBarr\\anaconda3\\envs\\blockchainDev\\lib\\site-packages\\sklearn\\base.py:577\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    575\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mValidation should be done on X, y or both.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    576\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 577\u001b[0m     X \u001b[39m=\u001b[39m check_array(X, input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[0;32m    578\u001b[0m     out \u001b[39m=\u001b[39m X\n\u001b[0;32m    579\u001b[0m \u001b[39melif\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_y:\n",
            "File \u001b[1;32mc:\\Users\\JBarr\\anaconda3\\envs\\blockchainDev\\lib\\site-packages\\sklearn\\utils\\validation.py:879\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    877\u001b[0m     \u001b[39m# If input is 1D raise error\u001b[39;00m\n\u001b[0;32m    878\u001b[0m     \u001b[39mif\u001b[39;00m array\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m--> 879\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    880\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mExpected 2D array, got 1D array instead:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39marray=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    881\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mReshape your data either using array.reshape(-1, 1) if \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    882\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39myour data has a single feature or array.reshape(1, -1) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mif it contains a single sample.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(array)\n\u001b[0;32m    884\u001b[0m         )\n\u001b[0;32m    886\u001b[0m \u001b[39mif\u001b[39;00m dtype_numeric \u001b[39mand\u001b[39;00m array\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mkind \u001b[39min\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mUSV\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    887\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    888\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdtype=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnumeric\u001b[39m\u001b[39m'\u001b[39m\u001b[39m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    889\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    890\u001b[0m     )\n",
            "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n 24 25 26 27 28 29].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
          ]
        }
      ],
      "source": [
        "degrees = np.arange(1, 9)\n",
        "    \n",
        "loss_train_list, loss_test_list = polynomial_regression(poly_reg_df, degrees)\n",
        "\n",
        "    # TODO START:\n",
        "    # Plot the polynomial degrees (x-axis) against loss_train_list (y-axis) and loss_test_list (y-axis) in a single plot, with different colors.\n",
        "    # Make sure to include x and y axis labels, legend as well as the title\n",
        "    # fit linear features\n",
        "\n",
        "plt.scatter(X, y, label='Training points')\n",
        "plt.plot(degrees, loss_train_list, label='Linear fit', linestyle='--')\n",
        "plt.plot(degrees, loss_test_list, label='Quadratic fit')\n",
        "plt.xlabel('Explanatory variable')\n",
        "plt.ylabel('Predicted or known target values')\n",
        "plt.legend(loc='upper left')\n",
        "\n",
        "plt.tight_layout()\n",
        "#plt.savefig('images/10_11.png', dpi=300)\n",
        "plt.show()\n",
        "    # TODO END"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-ByMGwo1TBS"
      },
      "source": [
        "**Attach the plot to your written homework solutions. Describe the trends in the plot obtained. Briefly explain the reasoning behind why this would happen.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78pB1H8vQ7qd"
      },
      "source": [
        "# **2. Logistic Regression** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ph7a-NIkNyQ9"
      },
      "source": [
        "## **2.1. Logistic Regression Implementation [18 pts, autograded]**\n",
        "\n",
        "Implement logistic regression with both L1 and L2 regularization by completing the LogisticRegression class.  \n",
        "\n",
        "Your class must implement the following API:\n",
        "\n",
        "* `__init__(alpha, tol, max_iter, theta_init, penalty, lambd)`\n",
        "* `sigmoid(x)`\n",
        "* `compute_cost(theta, X, y)`\n",
        "* `compute_gradient(theta, X, y)`\n",
        "* `has_converged(theta_old, theta_new)`\n",
        "* `fit(X, y)`\n",
        "* `predict_proba(X)`\n",
        "* `predict(X)`\n",
        "\n",
        "Note that these methods have already been defined correctly for you in the LogisticRegression class. **DO NOT** change the API.\n",
        "\n",
        "---\n",
        "\n",
        "### **2.1.1. Sigmoid Function [1 pt]**\n",
        "\n",
        "You should begin by implementing the `sigmoid` function.  As you may know, the sigmoid function $\\sigma(x)$ is mathematically defined as follows.\n",
        "\n",
        "> $\\sigma(x) = \\frac{1}{1\\ +\\ \\text{exp}(-x)}$\n",
        "\n",
        "**Be certain that your sigmoid function works with both vectors and matrices** --- for either a vector or a matrix, you function should perform the sigmoid function on every element.\n",
        "\n",
        "---\n",
        "\n",
        "### **2.1.2. Cost Function [5 pts]**\n",
        "\n",
        "The `compute_cost` function should compute the cost for a given $\\theta$ vector. The cost is a scalar value given by:\n",
        "\n",
        "> $\n",
        "\\mathcal{L}({\\theta}) = -\\sum_{i =1}^N [ y_i\\log(h_{{\\theta}}({x}_i)) + (1 - y_i)\\log(1 - h_{{\\theta}}({x}_i))]\n",
        "$\n",
        "\n",
        "where\n",
        "> $\n",
        "h_{\\theta}(x_{i}) = \\sigma(\\theta^{T}x_{i})\n",
        "$\n",
        "\n",
        "\n",
        "L1 Regularisation Loss:\n",
        ">$\n",
        "\\mathcal{L1}({\\theta}) = \\mathcal{L}({\\theta}) + \\lambda \\sum_{j = 1}^D  |{\\theta}_j|\n",
        "$\n",
        "\n",
        "L2 Regularisation Loss:\n",
        ">$\n",
        "\\mathcal{L2}({\\theta}) = \\mathcal{L}({\\theta}) + \\lambda \\sum_{j = 1}^D  {\\theta}_j^2 \n",
        "$\n",
        "\n",
        "$N$ is the number of training samples and $D$ is the number of features (excluding the intercept term). $\\theta$ is a $D + 1$ dimensional vector, with the first element being the intercept term. Note that we do not include the intercept in the regularization terms.\n",
        "\n",
        "---\n",
        "\n",
        "### **2.1.3. Gradient of the Cost Function [5 pts]**\n",
        "\n",
        "The `compute_gradient` function should compute the gradient of the cost function at a given $\\theta$.\n",
        "\n",
        "---\n",
        "\n",
        "### **2.1.4. Convergence Check [1 pt]**\n",
        "\n",
        "The `has_converged` function should return whether gradient descent algorithm has converged or not. Refer 2.1.5 for convergence condition.\n",
        " \n",
        "---\n",
        "\n",
        "### **2.1.5. Training [3 pts]**\n",
        "\n",
        "The `fit` method should train the model via gradient descent, relying on the cost and gradient functions. The trained weights/coefficients must be stored as `theta_`. The weights start as a zero vector. The weights and the corresponding cost after every gradient descent iteration must be stored in `hist_theta_` and `hist_cost_` respectively.\n",
        "\n",
        "* The gradient descent stops or converges when $\\theta$ stops changing or changes negligibly between consecutive iterations, i.e., when \n",
        "$\\| {\\theta}_\\mathit{new} -  {\\theta}_\\mathit{old} \\|_2 \\leq \\epsilon$, \n",
        "for some small $\\epsilon$ (e.g., $\\epsilon$ = 1E-4). $\\epsilon$ is stored as `tol` (short for tolerance). \n",
        "\n",
        "* To ensure that the function terminates, we should set a maximum limit for the number of gradient descent iterations irrespective of whether $\\theta$ converges or not. The limit is stored as `max_iter`.\n",
        "\n",
        "* `alpha` is the learning rate of the gradient descent algorithm.\n",
        "\n",
        "---\n",
        "\n",
        "### **2.1.6. Predict Probability [1 pt]**\n",
        "\n",
        "The `predict_probability` function should predict the probabilities that the data points in a given input data matrix belong to class 1.\n",
        "\n",
        "---\n",
        "\n",
        "### **2.1.7. Predict [2 pts]**\n",
        "\n",
        "The `predict` function should predict the classes of the data points in a given input data matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EeWSs7YNQ9ue"
      },
      "outputs": [],
      "source": [
        "class LogisticRegression:\n",
        "\n",
        "    \"\"\"\n",
        "    Logistic Regression (aka logit, MaxEnt) classifier.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    alpha: float, default=0.01\n",
        "        Learning rate\n",
        "    tol : float, default=0.0001\n",
        "        Tolerance for stopping criteria\n",
        "    max_iter : int, default=10000\n",
        "        Maximum number of iterations of gradient descent\n",
        "    theta_init: None (or) numpy.ndarray of shape (D + 1,)\n",
        "        The initial weights; if None, all weights will be zero by default\n",
        "    penalty : string, default = None\n",
        "        The type of regularization. The other acceptable options are l1 and l2\n",
        "    lambd : float, default = 1.0\n",
        "        The parameter regularisation constant (i.e. lambda)\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    theta_ : numpy.ndarray of shape (D + 1,)\n",
        "        The value of the coefficients after gradient descent has converged\n",
        "        or the number of iterations hit the maximum limit\n",
        "    hist_theta_ : numpy.ndarray of shape (num_iter, D + 1) where num_iter is the number of gradient descent iterations\n",
        "        Stores theta_ after every gradient descent iteration\n",
        "    hist_cost_ : numpy.ndarray of shape (num_iter,) where num_iter is the number of gradient descent iterations\n",
        "        Stores cost after every gradient descent iteration\n",
        "    \n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, alpha=0.01, tol=0.0001, max_iter=10000, theta_init=None, penalty = None, lambd = 1.0):\n",
        "\n",
        "        self.alpha = alpha\n",
        "        self.tol = tol\n",
        "        self.max_iter = max_iter\n",
        "        self.theta_init = theta_init\n",
        "        self.penalty = penalty\n",
        "        self.lambd = lambd\n",
        "\n",
        "        self.theta_ = None\n",
        "        self.hist_cost_ = None\n",
        "        self.hist_theta_ = None\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "\n",
        "        \"\"\"\n",
        "        Compute the sigmoid value of the argument.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x: numpy.ndarray\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        out: numpy.ndarray\n",
        "            The sigmoid value of x\n",
        "        \"\"\"\n",
        "\n",
        "        # TODO START: Complete the function\n",
        "        ...\n",
        "        # TODO END\n",
        "\n",
        "    def compute_cost(self, theta, X, y):\n",
        "\n",
        "        \"\"\"\n",
        "        Compute the cost/objective function.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        theta: numpy.ndarray of shape (D + 1,)\n",
        "            The coefficients\n",
        "        X: numpy.ndarray of shape (N, D + 1)\n",
        "            The features matrix\n",
        "        y: numpy.ndarray of shape (N,)\n",
        "            The target variable array\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        cost: float\n",
        "            The cost as a scalar value\n",
        "        \"\"\"\n",
        "        \n",
        "        # TODO START: Complete the function (should account for three cases - no penalty, l1 penalty, and l2 penalty)\n",
        "        # DO NOT use np.dot for this function as it can possibly return nan. Use a combination of np.nansum and np.multiply.\n",
        "        ...\n",
        "        # TODO END\n",
        "        \n",
        "    def compute_gradient(self, theta, X, y):\n",
        "\n",
        "        \"\"\"\n",
        "        Compute the gradient of the cost function.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        theta: numpy.ndarray of shape (D + 1,)\n",
        "            The coefficients\n",
        "        X: numpy.ndarray of shape (N, D + 1)\n",
        "            The features matrix\n",
        "        y: numpy.ndarray of shape (N,)\n",
        "            The target variable array\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        gradient: numpy.ndarray of shape (D + 1,)\n",
        "            The gradient values\n",
        "        \"\"\"\n",
        "\n",
        "        # TODO START: Complete the function (should account for three cases - no penalty, l1 penalty, and l2 penalty)\n",
        "        ...\n",
        "        # TODO END\n",
        "\n",
        "    def has_converged(self, theta_old, theta_new):\n",
        "\n",
        "        \"\"\"\n",
        "        Return whether gradient descent has converged.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        theta_old: numpy.ndarray of shape (D + 1,)\n",
        "            The weights prior to the update by gradient descent\n",
        "        theta_new: numpy.ndarray of shape (D + 1,)\n",
        "            The weights after the update by gradient descent\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        converged: bool\n",
        "            Whether gradient descent converged or not\n",
        "        \"\"\"\n",
        "\n",
        "        # TODO START: Complete the function\n",
        "        ...\n",
        "        # TODO END\n",
        "\n",
        "    def fit(self, X, y):\n",
        "\n",
        "        \"\"\"\n",
        "        Compute the coefficients using gradient descent and store them as theta_.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X: numpy.ndarray of shape (N, D)\n",
        "            The features matrix\n",
        "        y: numpy.ndarray of shape (N,)\n",
        "            The target variable array\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        Nothing\n",
        "        \"\"\"\n",
        "\n",
        "        N, D = X.shape\n",
        "\n",
        "        # Adding a column of ones at the beginning for the bias term\n",
        "        ones_col = np.ones((N, 1))\n",
        "        X = np.hstack((ones_col, X))\n",
        "        \n",
        "        # Initializing the weights\n",
        "        if self.theta_init is None:\n",
        "            theta_old = np.zeros((D + 1,))\n",
        "        else:\n",
        "            theta_old = self.theta_init\n",
        "\n",
        "        # Initializing the historical weights matrix\n",
        "        # Remember to append this matrix with the weights after every gradient descent iteration\n",
        "        self.hist_theta_ = np.array([theta_old])\n",
        "\n",
        "        # Computing the cost for the initial weights\n",
        "        cost = self.compute_cost(theta_old, X, y)\n",
        "\n",
        "        # Initializing the historical cost array\n",
        "        # Remember to append this array with the cost after every gradient descent iteration\n",
        "        self.hist_cost_ = np.array([cost])\n",
        "        \n",
        "        # TODO START: Complete the function\n",
        "        ...\n",
        "        # TODO END\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "\n",
        "        \"\"\"\n",
        "        Predict the probabilities that the data points in X belong to class 1.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X: numpy.ndarray of shape (N, D)\n",
        "            The features matrix\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        y_hat: numpy.ndarray of shape (N,)\n",
        "            The predicted probabilities that the data points in X belong to class 1\n",
        "        \"\"\"\n",
        "\n",
        "        N = X.shape[0]\n",
        "        X = np.hstack((np.ones((N, 1)), X))\n",
        "        \n",
        "        # TODO START: Complete the function\n",
        "        ...\n",
        "        # TODO END\n",
        "\n",
        "    def predict(self, X):\n",
        "\n",
        "        \"\"\"\n",
        "        Predict the classes of the data points in X.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X: numpy.ndarray of shape (N, D)\n",
        "            The features matrix\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        y_pred: numpy.ndarray of shape (N,)\n",
        "            The predicted class of the data points in X\n",
        "        \"\"\"\n",
        "\n",
        "        # TODO START: Complete the function\n",
        "        ...\n",
        "        # TODO END"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-VyGzobU4d3"
      },
      "outputs": [],
      "source": [
        "def test_log_reg_sigmoid(StudentLogisticRegression):\n",
        "    \n",
        "    student_lr_clf = StudentLogisticRegression()\n",
        "    test_case = np.array([ 1.62434536, -0.61175641, -0.52817175, -1.07296862,  0.86540763])\n",
        "    student_ans = student_lr_clf.sigmoid(test_case)\n",
        "    required_ans = np.array([0.83539354, 0.35165864, 0.3709434 , 0.25483894, 0.70378922])\n",
        "\n",
        "    assert np.linalg.norm(student_ans - required_ans) <= 1e-2\n",
        "\n",
        "if NOTEBOOK:\n",
        "    test_log_reg_sigmoid(LogisticRegression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wvem30ziTrrs"
      },
      "outputs": [],
      "source": [
        "# PennGrader Grading Cell\n",
        "if NOTEBOOK:\n",
        "    grader.grade(test_case_id = 'test_log_reg_sigmoid', answer = LogisticRegression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IR5a5eEsVbpj"
      },
      "outputs": [],
      "source": [
        "def test_log_reg_compute_cost(StudentLogisticRegression):\n",
        "    \n",
        "    test_case_theta = np.array([ 1.62434536, -0.61175641])\n",
        "    test_case_X = np.array([[ 1.62434536, -0.61175641],\n",
        "                            [-0.52817175, -1.07296862],\n",
        "                            [ 0.86540763, -2.3015387 ],\n",
        "                            [ 1.74481176, -0.7612069 ],\n",
        "                            [ 0.3190391,  -0.24937038]])\n",
        "    test_case_y = np.array([1, 1, 0, 0, 1])\n",
        "\n",
        "    student_lr_clf = StudentLogisticRegression()\n",
        "    student_ans = student_lr_clf.compute_cost(test_case_theta, test_case_X, test_case_y)\n",
        "    required_ans = 7.467975765663204\n",
        "\n",
        "    assert np.abs(student_ans - required_ans) <= 1e-2\n",
        "\n",
        "    student_lr_clf = StudentLogisticRegression(penalty=\"l1\", lambd=0.1)\n",
        "    student_ans = student_lr_clf.compute_cost(test_case_theta, test_case_X, test_case_y)\n",
        "    required_ans = 7.52915138076548\n",
        "\n",
        "    assert np.abs(student_ans - required_ans) <= 1e-2\n",
        "\n",
        "    student_lr_clf = StudentLogisticRegression(penalty=\"l2\", lambd=0.1)\n",
        "    student_ans = student_lr_clf.compute_cost(test_case_theta, test_case_X, test_case_y)\n",
        "    required_ans = 7.505400330283089\n",
        "    assert np.abs(student_ans - required_ans) <= 1e-2\n",
        "\n",
        "if NOTEBOOK:\n",
        "    test_log_reg_compute_cost(LogisticRegression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CAWvDLS0_Ejb"
      },
      "outputs": [],
      "source": [
        "# PennGrader Grading Cell\n",
        "if NOTEBOOK:\n",
        "    grader.grade(test_case_id = 'test_log_reg_compute_cost', answer = LogisticRegression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NP5ehbmv6aQa"
      },
      "outputs": [],
      "source": [
        "def test_log_reg_compute_gradient(StudentLogisticRegression):\n",
        "    \n",
        "    test_case_theta = np.array([ 1.62434536, -0.61175641])\n",
        "    test_case_X = np.array([[ 1.62434536, -0.61175641],\n",
        "                            [-0.52817175, -1.07296862],\n",
        "                            [ 0.86540763, -2.3015387 ],\n",
        "                            [ 1.74481176, -0.7612069 ],\n",
        "                            [ 0.3190391,  -0.24937038]])\n",
        "    test_case_y = np.array([1, 1, 0, 0, 1])\n",
        "\n",
        "    student_lr_clf = StudentLogisticRegression()\n",
        "    student_ans = student_lr_clf.compute_gradient(test_case_theta, test_case_X, test_case_y)\n",
        "    required_ans = np.array([ 2.60573737, -2.20203139])\n",
        "\n",
        "    assert np.linalg.norm(student_ans - required_ans) <= 1e-2\n",
        "\n",
        "    student_lr_clf = StudentLogisticRegression(penalty=\"l1\", lambd=0.1)\n",
        "    student_ans = student_lr_clf.compute_gradient(test_case_theta, test_case_X, test_case_y)\n",
        "    required_ans = np.array([ 2.60573737, -2.30203139])\n",
        "\n",
        "    assert np.linalg.norm(student_ans - required_ans) <= 1e-2\n",
        "\n",
        "    student_lr_clf = StudentLogisticRegression(penalty=\"l2\", lambd=0.1)\n",
        "    student_ans = student_lr_clf.compute_gradient(test_case_theta, test_case_X, test_case_y)\n",
        "    required_ans = np.array([ 2.60573737, -2.32438267])\n",
        "\n",
        "    assert np.linalg.norm(student_ans - required_ans) <= 1e-2\n",
        "\n",
        "if NOTEBOOK:\n",
        "    test_log_reg_compute_gradient(LogisticRegression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oehKOwNn_Ion"
      },
      "outputs": [],
      "source": [
        "# PennGrader Grading Cell\n",
        "if NOTEBOOK:\n",
        "    grader.grade(test_case_id = 'test_log_reg_compute_gradient', answer = LogisticRegression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o5Jx2O4M79sd"
      },
      "outputs": [],
      "source": [
        "def test_log_reg_has_converged(StudentLogisticRegression):\n",
        "\n",
        "    student_lr_clf = StudentLogisticRegression()\n",
        "    test_case_theta_old = np.array([ 1.62434536, -0.61175641])\n",
        "    test_case_theta_new = np.array([1.624345, -0.611756])\n",
        "    student_ans = student_lr_clf.has_converged(test_case_theta_old, test_case_theta_new)\n",
        "    required_ans = True\n",
        "\n",
        "    assert student_ans == required_ans\n",
        "\n",
        "if NOTEBOOK:\n",
        "    test_log_reg_has_converged(LogisticRegression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AX_D-6Ag_K76"
      },
      "outputs": [],
      "source": [
        "# PennGrader Grading Cell\n",
        "if NOTEBOOK:\n",
        "    grader.grade(test_case_id = 'test_log_reg_has_converged', answer = LogisticRegression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sZf2uH61FbYM"
      },
      "outputs": [],
      "source": [
        "def test_log_reg_fit(StudentLogisticRegression):\n",
        "    \n",
        "    student_lr_clf = StudentLogisticRegression(max_iter=5)\n",
        "    test_case_X = np.array([[ 1.62434536, -0.61175641],\n",
        "                            [-0.52817175, -1.07296862],\n",
        "                            [ 0.86540763, -2.3015387 ],\n",
        "                            [ 1.74481176, -0.7612069 ],\n",
        "                            [ 0.3190391,  -0.24937038]])\n",
        "    test_case_y = np.array([1, 1, 0, 0, 1])\n",
        "    student_lr_clf.fit(test_case_X, test_case_y)\n",
        "    student_ans = student_lr_clf.hist_theta_\n",
        "    required_ans = np.array([[ 0.        ,  0.        ,  0.        ],\n",
        "                             [ 0.005     , -0.00597503,  0.00564325],\n",
        "                             [ 0.01006813, -0.01184464,  0.0111865 ],\n",
        "                             [ 0.01520121, -0.01761226,  0.01663348],\n",
        "                             [ 0.02039621, -0.02328121,  0.02198778],\n",
        "                             [ 0.02565018, -0.0288547 ,  0.02725288]])\n",
        "\n",
        "    assert np.linalg.norm(student_ans - required_ans) <= 1e-2\n",
        "\n",
        "if NOTEBOOK:\n",
        "    test_log_reg_fit(LogisticRegression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F3geNT88Ivqm"
      },
      "outputs": [],
      "source": [
        "# PennGrader Grading Cell\n",
        "if NOTEBOOK:\n",
        "    grader.grade(test_case_id = 'test_log_reg_fit', answer = grader_serialize(LogisticRegression))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gKywIL8gIyw0"
      },
      "outputs": [],
      "source": [
        "def test_log_reg_predict_proba(StudentLogisticRegression):\n",
        "    \n",
        "    student_lr_clf = StudentLogisticRegression(max_iter=5)\n",
        "    test_case_X = np.array([[ 1.62434536, -0.61175641],\n",
        "                            [-0.52817175, -1.07296862],\n",
        "                            [ 0.86540763, -2.3015387 ],\n",
        "                            [ 1.74481176, -0.7612069 ],\n",
        "                            [ 0.3190391,  -0.24937038]])\n",
        "    test_case_y = np.array([1, 1, 0, 0, 1])\n",
        "    student_lr_clf.fit(test_case_X, test_case_y)\n",
        "    student_ans = student_lr_clf.predict_proba(test_case_X)\n",
        "    required_ans = np.array([0.49052814, 0.5029122 , 0.48449386, 0.48864172, 0.50241207])\n",
        "\n",
        "    assert np.linalg.norm(student_ans - required_ans) <= 1e-2\n",
        "\n",
        "if NOTEBOOK:\n",
        "    test_log_reg_predict_proba(LogisticRegression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WUzQe-7BLYAX"
      },
      "outputs": [],
      "source": [
        "# PennGrader Grading Cell\n",
        "if NOTEBOOK:\n",
        "    grader.grade(test_case_id = 'test_log_reg_predict_proba', answer = grader_serialize(LogisticRegression))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nuvyk1SbNv_o"
      },
      "outputs": [],
      "source": [
        "def test_log_reg_predict(StudentLogisticRegression):\n",
        "\n",
        "    student_lr_clf = StudentLogisticRegression(max_iter=5)\n",
        "    np.random.seed(1)\n",
        "    test_case_X = np.random.randn(50, 2)\n",
        "    test_case_y = np.random.randint(0, 2, 50)\n",
        "    student_lr_clf.fit(test_case_X, test_case_y)\n",
        "    student_ans = student_lr_clf.predict(test_case_X)\n",
        "    required_ans = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,\n",
        "                             0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1])\n",
        "\n",
        "    assert np.mean(np.abs(student_ans - required_ans)) <= 0.02\n",
        "\n",
        "if NOTEBOOK:\n",
        "    test_log_reg_predict(LogisticRegression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MBZk4fwJOdI2"
      },
      "outputs": [],
      "source": [
        "# PennGrader Grading Cell\n",
        "if NOTEBOOK:\n",
        "    grader.grade(test_case_id = 'test_log_reg_predict', answer = grader_serialize(LogisticRegression))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAVp1ZEkL2l9"
      },
      "source": [
        "## **2.2. Effect of learning rate on gradient descent [5 pts, manually graded]**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxNxtdHnRLws"
      },
      "source": [
        "Run the below cell to download the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c6W4LVZgMl3g"
      },
      "outputs": [],
      "source": [
        "if NOTEBOOK:\n",
        "    \n",
        "    if not os.path.exists(\"cis519_hw2_admit.csv\"):\n",
        "        !gdown --id 1CSD1vA9qZucuevxCuaOwr91tBaZcjNNh\n",
        "    \n",
        "    train_df = pd.read_csv(\"cis519_hw2_admit.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqzaeWj1PIa-"
      },
      "source": [
        "The dataset contains two features - scores in two exams and the target variable is whether the student was admitted into a college or not. Your task for this question is to use this dataset and plot the variation of cost function with respect to the number of gradient descent iterations for different learning rates. Perform the following steps.\n",
        "\n",
        "1. Scale the features using StandardScaler\n",
        "2. For each of the learning rates - {0.001, 0.01, 0.1, 0.25}, fit a logistic regression model to the scaled data by running a maximum of 100 iterations of gradient descent with L2 penalty and $\\lambda$ as 0.001.\n",
        "3. Show the variation of the cost (stored in `hist_cost_`) with respect to the number of iterations for all the learning rates in the same plot.\n",
        "\n",
        "Submit the plot along with the written homework solutions. The plot should have an appropriate title, axes labels, and legend. Briefly comment on the effect of increasing learning rate and what would be the best learning rate among the four values based on the plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tR1ZrrxYM36e"
      },
      "outputs": [],
      "source": [
        "if NOTEBOOK:\n",
        "    # STUDENT CODE STARTS:\n",
        "    ...\n",
        "    # STUDENT CODE ENDS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYRykDPr9U8A"
      },
      "source": [
        "Download the .ipynb notebook and submit on Gradescope."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.13 ('blockchainDev')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "1e9c44cc3bc95fb73b0aae724063041b6d6060f5ce93be071123fdfeed5e731e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
